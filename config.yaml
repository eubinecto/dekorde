# --- config for training a transformer --- #
transformer:
  ver: overfit
  desc: "Fully overfitted to kor2eng dataset. Not sure how many epochs we may need, so we first start with 30."
  hidden_size: 512
  ffn_size: 512
  heads: 32
  depth: 3
  max_epochs: 30
  max_length: 145
  batch_size: 64
  lr: 0.0001
  dropout: 0.0
  seed: 410
  shuffle: true
  tokenizer: wp

# --- config for building a tokenizer --- #
tokenizer:
  ver: wp
  vocab_size: 20000
  pad: "[PAD]"
  pad_id: 0
  unk: "[UNK]"
  unk_id: 1
  bos: "[BOS]"
  bos_id: 2
  eos: "[EOS]"
  eos_id: 3

