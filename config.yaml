# --- config for training a transformer --- #
transformer:
  desc: "Fully overfitted to kor2eng dataset. Not sure how many epochs we may need, so we first start with 30."
  best: "transformer:overfit"
  hidden_size: 512
  ffn_size: 512
  heads: 32
  depth: 3
  max_length: 145
  batch_size: 3
  lr: 0.0001
  dropout: 0.0
  seed: 410
  shuffle: true
  tokenizer: "tokenizer:wp"
  monitor: Validation/Loss_epoch
  mode: min

# --- config for building a tokenizer --- #
tokenizer:
  ver: wp
  vocab_size: 20000
  pad: "[PAD]"
  pad_id: 0
  unk: "[UNK]"
  unk_id: 1
  bos: "[BOS]"
  bos_id: 2
  eos: "[EOS]"
  eos_id: 3

# --- config for building a dataset --- #
kor2eng:
  ver: ...