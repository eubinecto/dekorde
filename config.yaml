# --- config for training a transformer --- #
transformer:
  hidden_size: 512
  ffn_size: 2048
  heads: 8
  depth: 5
  max_length: 150
  lr: 0.01
  dropout: 0.1
  tokenizer: "tokenizer:v20"
  # for dataloader
  seed: 410
  shuffle: true
  # for ADAM
  eps: 0.000000001
  betas: [0.9, 0.98]
  weight_decay: 0.0005
  # for ReduceOnPlateau
  monitor: train/loss_epoch
  mode: min
  patience: 3
  cooldown: 1

# --- config for building a tokenizer --- #
tokenizer:
  algorithm: wordpiece
  kor2eng: "kor2eng:v0"
  vocab_size: 16000
  pad: "[PAD]"
  pad_id: 0
  unk: "[UNK]"
  unk_id: 1
  bos: "[BOS]"
  bos_id: 2
  eos: "[EOS]"
  eos_id: 3

# --- config for recommended versions of artifacts --- #
recommended:
  transformer: "model-xb8gtf6k:v25"
  tokenizer: "tokenizer:v20"
  kor2eng: "kor2eng:v0"
